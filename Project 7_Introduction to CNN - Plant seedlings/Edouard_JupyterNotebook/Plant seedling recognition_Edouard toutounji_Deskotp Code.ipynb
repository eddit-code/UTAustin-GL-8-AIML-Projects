{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-Libraries\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Data loading\n",
    "\n",
    "train_path ='/content/drive/My Drive/CNN_project/train.zip'\n",
    "!mkdir temp_train\n",
    "\n",
    "from zipfile import ZipFile\n",
    "with ZipFile ( train_path, 'r') as zip:\n",
    "    zip.extractall('/content/drive/My Drive/CNN_project/temp_train')\n",
    "    \n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# The upper Zip extraction wasn't working no matter what !! \n",
    "# I even uploaded the folder of unzipped pics 'train' and replaced the lower path as:\n",
    "\n",
    "# path_to_images ='content/drive/My Drive/CNN_project/train/*/*.png'\n",
    "# But still to no avail !  :-( \n",
    "\n",
    "# THE CODE from here down is to the best of my guessing on how this CNN project should evolve.\n",
    "# It was done on my desktop as I lost hope with reading the images into to the code :-(\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "path_to_images ='content/drive/My Drive/CNN_project/temp_train/*/*.png'\n",
    "images = glob(path_to_images)\n",
    "\n",
    "trainImg   = []\n",
    "trainLabel = []\n",
    "\n",
    "count=1\n",
    "num = len(images)\n",
    "for img in images:\n",
    "    print(str(j) + '/' + str(num) , end= '\\r' )\n",
    "    trainImg.append ( cv2.resize(cv2.imread(img),(128,128)) )\n",
    "    trainLabel.append (img.split('/')[-2])\n",
    "    count+=1\n",
    "    \n",
    "trainImg   = np.asarray(trainImg)\n",
    "trainLabel = pd.DataFrame(trainLabel)\n",
    "\n",
    "print(trainImg.shape)\n",
    "print(trainLabel.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3- Show some Original images\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(trainImg[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Blur the images \n",
    "\n",
    "blurred_trainImg = []\n",
    "\n",
    "for img in trainImg:\n",
    "    blurImg = cv2.GaussianBlur(img ,(5,5), 0)\n",
    "    blurred_trainImg.append(blurImg)\n",
    "\n",
    "    \n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(blurred_trainImg[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - Normalising the blurred training set\n",
    "\n",
    "blurred_trainImg = blurred_trainImg /255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 7- One hot encoding of the Labels set\n",
    "    \n",
    "labels = preprocessing.LabelEncoder()\n",
    "labels.fit(trainLabel[0])\n",
    "print('Classes' + str(labels.classes))\n",
    "\n",
    "encodedlabels = labels.transform(trainlabel[0])\n",
    "clearalllabels = to_categorically(encodedlabels )\n",
    "classes = clearalllabels.shape[1]\n",
    "\n",
    "print(str(classes))\n",
    "trainlabel[0].value_counts.plot(kind= 'bar')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8- X and y \n",
    "\n",
    "X = blurred_trainImg\n",
    "y = clearalllabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9- train , validation , test sets : 70% , 15%, 15%\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size = 0.3, random_state=0, stratify = y)\n",
    "\n",
    "X_val, X_test, y_val, y_test     = train_test_split(X_temp, y_temp, test_size = 0.5, random_state=0, stratify = y_temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10- CNN Architecture \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Conv2D(filters=64, kernel_size=(5, 5), input_shape=(128, 128, 3), activation='relu'))\n",
    "\n",
    "# 3 Convolution layers\n",
    "model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# 2 fully connected layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# One output layer\n",
    "model.add(Dense(12, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 - Fitting the model \n",
    "\n",
    "model.fit( x=X_train, y=y_train,\n",
    "          batch_size=32, \n",
    "          epochs=15 , \n",
    "          validation_data=(X_val, y_val),\n",
    "          shuffle=True,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12 - Model Evaluation \n",
    "\n",
    "scores = model.evaluate ( X_test , y_test, verbose=1)\n",
    "print( 'Test loss: ', score[0])\n",
    "print( 'Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13- Model Prediction  and Confusion matrix\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('===Confusion matrix===')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "\n",
    "print('===Classification report===')\n",
    "print(classification_report(y_test, y_pred))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14- Visualise some predictions\n",
    "\n",
    "for i in [2,3,33,36,59]:\n",
    "    plt.imshow(X_test[i].reshape(128,128,3))\n",
    "    y_pred = model.predict(X_test[i], reshape(1,128,128,3))\n",
    "    print('Predicted label: ', y_pred.argmax())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
