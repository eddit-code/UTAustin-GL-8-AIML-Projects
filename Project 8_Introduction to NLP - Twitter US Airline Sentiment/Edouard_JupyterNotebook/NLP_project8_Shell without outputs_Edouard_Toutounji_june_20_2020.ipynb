{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjkC0Bm5hSPx"
   },
   "source": [
    "#NLP - Project 8 : Edouard Toutounji : June_19_2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KuuI-ojZjqwI"
   },
   "source": [
    "## 1- Essential preprocessing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90K02EsHAlwI"
   },
   "outputs": [],
   "source": [
    "# NLP - Project8 : Edouard Toutounji June 20_2020\n",
    "\n",
    "# 1- Essential preprocessing libraries \n",
    "\n",
    "import contractions\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pmftYJKjpC2"
   },
   "source": [
    "## 2- Data load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w24u-57yApe7"
   },
   "outputs": [],
   "source": [
    "# 2.1 - Data load \n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "data = pd.read_csv ('/content/drive/My Drive/Colab Notebooks/Tweets.csv')\n",
    "data.shape\n",
    "data.info()\n",
    "data.head()\n",
    "data.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-3rObqriCuw"
   },
   "outputs": [],
   "source": [
    "# 2.2 'airline_sentiment' needs to be encoded to integers\n",
    "\n",
    "data['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3EkaVTokRUC"
   },
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "data['airline_sentiment_coded'] = labelencoder.fit_transform(data['airline_sentiment'])\n",
    "data.airline_sentiment_coded.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SpMKCmcLkXTy"
   },
   "outputs": [],
   "source": [
    "# 2.3 Display Expansion and keeping only the 2 columns needed\n",
    "\n",
    "\n",
    "# display's expansion \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "# keep columns in question\n",
    "data = data[['airline_sentiment_coded','text']]\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PoKAzmQkm9Hb"
   },
   "source": [
    "## 3- Elementary preprocessing functions ,  and then an encapsulating normaling function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0uxSeImrk99k"
   },
   "outputs": [],
   "source": [
    "# 3- Elementary preprocessing functions ,  and then and encapsulating normaling function.\n",
    "\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    " # # # # # # # # # # # # # # # # # # # # # # # # \n",
    " # The 9 elementary functions:\n",
    "\n",
    "def strip_html(words):\n",
    "    temp = BeautifulSoup( words , 'html.parser')\n",
    "    return temp.get_text()\n",
    "\n",
    "def replace_contractions(words):\n",
    "    temp = contractions.fix(words)\n",
    "    return temp\n",
    "\n",
    "def remove_numbers(words):\n",
    "    temp = re.sub(r'\\d+', '', words)\n",
    "    return temp\n",
    "\n",
    "def tokenize_text(words):\n",
    "    temp = nltk.word_tokenize(words)\n",
    "    return temp\n",
    "\n",
    "def remove_non_ascii (words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii','ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation (words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub( r'[^\\w\\s]','',word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords (words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_words (words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "            new_words.append(lemmatizer.lemmatize(word))\n",
    "    return new_words\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # \n",
    "\n",
    "\n",
    "# The encapsulating Normalising function \n",
    "\n",
    "def normalize(words):\n",
    "    words = strip_html(words)\n",
    "    words = replace_contractions(words)\n",
    "    words = remove_numbers(words)\n",
    "    words = tokenize_text(words)\n",
    "    words = remove_non_ascii (words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation (words)\n",
    "    words = remove_stopwords (words)\n",
    "    words = lemmatize_words (words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# normalise will iterate on all the cells of the data['text'] column\n",
    "\n",
    "for i,row in data.iterrows():\n",
    "    words = data.at[i , 'text']\n",
    "    words = normalize (words)\n",
    "    data.at[i , 'text'] = words\n",
    "    \n",
    "data.head(10)\n",
    "                                                                                       \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6nKLt_HHo3qG"
   },
   "source": [
    "##4- Libraries for vectorisation and then ML classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WMDfcFM4o5xb"
   },
   "outputs": [],
   "source": [
    "# 4- Libraries for vectorisation and then ML classification \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wpIxPWBnMUW"
   },
   "source": [
    "## 5- Random Forest Model using CountVectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpKWC5E8nxXI"
   },
   "outputs": [],
   "source": [
    "# 5- Random Forest Model using CountVectoriser\n",
    "\n",
    "vectorizer = CountVectorizer( max_features = 2000)\n",
    "\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "X = X.toarray()\n",
    "X.shape\n",
    "\n",
    "y = data['airline_sentiment_coded']\n",
    "y.shape\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 7)\n",
    "\n",
    "\n",
    "# Fitting the model \n",
    "forest = RandomForestClassifier(n_estimators = 20 , n_jobs=4)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Accuracy Score on the whole Data \n",
    "print('Accuracy score on the whole Data')\n",
    "print(np.mean(cross_val_score(forest, X, y , cv =20)))\n",
    "\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "y_pred = forest.predict(X_test)\n",
    "cm = confusion_matrix (y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "df_cm = pd.DataFrame( cm, index = [i for i in '012'] , columns = [i for i in '012'])\n",
    "plt.figure( figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OauJd_RCpoWH"
   },
   "source": [
    "##6- Random Forest Model using TfidfVectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yg7BEOJRpGzx"
   },
   "outputs": [],
   "source": [
    "# 6- Random Forest Model using TfidfVectoriser\n",
    "\n",
    "vectoriser = TfidfVectorizer( max_features = 2000)\n",
    "\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "X = X.toarray()\n",
    "X.shape\n",
    "\n",
    "y = data['airline_sentiment_coded']\n",
    "y.shape\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 7)\n",
    "\n",
    "\n",
    "# Fitting the model \n",
    "forest = RandomForestClassifier(n_estimators = 20 , n_jobs=4)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Accuracy Score on the whole Data \n",
    "print('Accuracy score on the whole Data')\n",
    "print(np.mean(cross_val_score(forest, X, y , cv =20)))\n",
    "\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "y_pred = forest.predict(X_test)\n",
    "cm = confusion_matrix (y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "df_cm = pd.DataFrame( cm, index = [i for i in '012'] , columns = [i for i in '012'])\n",
    "plt.figure( figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OMcuuc_Aqb8B"
   },
   "source": [
    "##7- Final thoughts on CountVectoriser vs. TfidfVectoriser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ekIETrDCqurw"
   },
   "source": [
    "Tfdif is barely slighlty better at classifying the text.\n",
    "\n",
    "Accuracy scores:\n",
    "*   CountVectorizer: 0.7155737704918033\n",
    "*   TfIdfVectorizer: 0.7185109289617486\n",
    "\n",
    "\n",
    "\n",
    "Also the results took a long time to be processed on Colab with the hyperpapramters above.\n",
    "\n",
    "________________________________________________________________\n",
    "\n",
    "The initial trials for both Vectotization approaches was initially run with half the above:\n",
    "*   max_features = 1000\n",
    "*   n_estimators = 10\n",
    "*   cv = 10\n",
    "\n",
    "The results were faster but slighly less in terms of accuracy, both approaching the 70% correct classification rate.\n",
    "\n",
    "Accuracy scores:\n",
    "*   CountVectorizer: 0.6895491803278688\n",
    "*   TfIdfVectorizer: 0.6962431693989071\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Thank you GL team, the journey was not easy but so much worth it!\n",
    "\n",
    "Edouard Toutounji\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3UADhhlrEFV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP__project8____Edouard Toutounji____june_20_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
